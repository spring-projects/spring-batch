<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.4//EN"
"http://www.oasis-open.org/docbook/xml/4.4/docbookx.dtd">
<chapter id="spring-batch-infrastructure">
  <title>ItemReaders and ItemWriters</title>

  <section>
    <title id="i-0.spring-batch-infrastructure-overview">Introduction</title>

    <para>All batch processing can be described in its most simple form as
    reading in large ammounts of data, performing some type of calculation or
    transformation, and writing the result back out. Spring Batch provides two
    key interfaces to help perform bulk reading and writing: ItemReader and
    ItemWriter</para>
  </section>

  <section>
    <title id="infrastructure.1">ItemReader</title>

    <para>Although a simple concept, <emphasis
    role="bold">ItemReader</emphasis>s are the means for providing data from
    many different types of input. The most general examples include:
    <itemizedlist>
        <listitem>
          <para>Flat File- Flat File Item Readers read lines of data from a
          flat file that typically describe records with fields of data
          defined by fixed positions in the file or delimited by some special
          character (e.g. comma).</para>
        </listitem>

        <listitem>
          <para>XML - XML ItemReaders process XML independently of
          technologies used for parsing, mapping and validating objects. Input
          data allows for the validation of and XML file against and XSD
          schema.</para>
        </listitem>

        <listitem>
          <para>SQL - A database resource accessed that returns resultsets
          that can be mapped to objects for processing. The default SQL Input
          Sources invoke a RowMapper to return objects, keep track of the
          current row if restart is required, basic statistics, and some
          transaction enhancements that will be explained later.</para>
        </listitem>

        <listitem>
          <para>JMS - An ItemReader for JMS using JmsTemplate. The template
          should have a default destination, which will be used to provide
          items in read(). If a recovery step is needed, set the error
          destination and the item will be sent there if processing fails in
          an external retry.</para>
        </listitem>
      </itemizedlist>There are many more possbilities, but we'll focus on the
    basic ones for this chapter. A complete list of all available ItemReaders
    can be found in Appendix A.</para>

    <para>The Item Reader is a basic interface for generic input
    operations:</para>

    <programlisting>public interface ItemReader {

  Object read() throws Exception;

  void mark() throws MarkFailedException;

  void reset() throws ResetFailedException;
}
</programlisting>

    <para>The read() method defines the most essential contract of the
    ItemReader, calling it returns one Item, returning null if no more items
    are left. An item might represent a line in a file, a row in a database,
    or an element in an xml file. It is generally expected that these will be
    mapped to a useable domain object (i.e. Trade or Foo, etc) but there is no
    requirement in the contract to do so.</para>

    <para>mark() and reset() are important methods due to the transactional
    nature of batch processing. Mark() will be called before reading begins.
    Calling reset() at anytime will position the ItemReader to its position
    when Mark() was last called. The semantics are very similar to
    java.io.Reader.</para>
  </section>

  <section>
    <title id="infrastructure.1.4">ItemWriter</title>

    <para>Item Writers are similar in functionality to an ItemReader with the
    exception that the operations are reversed. They still need to be located,
    opened and closed but they differ in the case that we write out, rather
    than reading in. In the case of databases or queues these may be inserts,
    updates or sends. The format of the serialization of the output source is
    specific for every batch job.</para>

    <para>As with ItemReader, ItemWriter is a fairly generic interface:</para>

    <programlisting>public interface ItemWriter {

  void write(Object item) throws Exception;

  void flush() throws FlushFailedException;

  void clear() throws ClearFailedException;
}
</programlisting>

    <para>As with read() on ItemReader, write provides the basic contract of
    ItemWriter, it will attempt to write out the item passed in as long as it
    is open. As with mark() and reset(), flush() and clear() are necessary due
    to the nature of batch processing. Because it is generally expected that
    items will be 'batched' together into a chunk, and then output, it is
    expected that an ItemWriter will perform some type of buffering. flush()
    will empty the buffer by actually writing the items out, whereas clear
    will simply throw the contents of the buffer away. In most cases, a Step
    implementation will call flush() before a commit and clear() in case of
    rollback.</para>
  </section>

  <section>
    <title>ItemStream</title>

    <para>Both ItemReaders and ItemWriters serve their individual purposes
    well, but there is a common concern among both of them that necessitates
    another interface. In general, as part of the scope of a batch job,
    readers and writers need to be opened, closed, and require a mechanism for
    persisting state:</para>

    <programlisting>public interface ItemStream {

  void open(ExecutionContext executionContext) throws StreamException;

  void update(ExecutionContext executionContext);
  
  void close(ExecutionContext executionContext) throws StreamException;
}
</programlisting>

    <para>Before describing each method, it's worth breifly mentioning the
    ExecutionContext. An ExecutionContext is created for Each StepExecution to
    allow users to store the state of a particular execution, with the
    expectation that it will be returned if the same JobInstance is started
    again. For those familiar with Quartz, the semantics are very similar to a
    Quartz JobDataMap. Open() should be called before any calls to read or
    write and is expected to open any resources such as files or obtain
    connections. As mentioned before, if expected data is found in the
    ExecutionContext, it may be used to start the ItemReader or ItemWriter at
    a location other than its initial state. Converely, close will be called
    to ensure any resources allocated during open will be released safely.
    Update() is called primarily to ensure that any state currently being held
    is loaded into the provided ExecutionContext. In most cases, this method
    will be called before committing, to ensure that the current state is
    persisted in the database before commit.</para>
  </section>

  <section>
    <title id="infrastructure.1.1">List Item Readers and Common Custom Item
    Reader Behavior</title>

    <para>The <emphasis role="bold">ListItemReader</emphasis>, as mentioned
    above, is useful for testing and probably not too useful as something used
    in typical batch processing. One instructive use is to see how narrow the
    responsiblity of ItemReaders are. They simply provide a method that allows
    us to continue reading items until the items are exhausted much like an
    iterator. In addition,, it is expected that projects will create custom
    Item Readers. As a means of illustrating the standard properties and
    behaviors of other framework-provided ItemReaders like mapping
    unstructured items into objects through the use of tokenizing we will
    extend the ListItemReader to supporting mapping. The ItemReader interface
    defines a single method called <emphasis role="bold">read()</emphasis>.
    The <emphasis role="bold">read()</emphasis> method returns the next object
    to be provided, much like an iterator. The definition of this method will
    contain the logic that decides what object to return, performs any object
    construction or other work that needs to occur, and finally returns the
    object. We inherit this behavior from ListItemReader. We will add two
    methods, <emphasis role="bold"> setFieldSetMapper()</emphasis>, to enable
    the mapping behavior and <emphasis role="bold">setTokenizer()</emphasis>,
    to enabling parsing of List Items. It this example the items in the list
    are a simple array of delimited strings..</para>

    <para>Here is our custom list item Reader that supplies mapping or binding
    behavior as follows: <programlisting>
        protected static class ListPlayerReader extends ListItemReader {
                private FieldSetMapper fieldSetMapper;
                private LineTokenizer tokenizer = null;
                
                public ListPlayerReader(List list) {
                        super(list);
                }

                public void setFieldSetMapper(FieldSetMapper fieldSetMapper) {
                        this.fieldSetMapper = fieldSetMapper;
                }
                
                public void setTokenizer(LineTokenizer tokenizer) {
                        this.tokenizer = tokenizer; 
                }

                
        }
        </programlisting></para>

    <para>We will tag it as an Player Reader for reasons you'll see next as we
    map Player objects from input strings. In this example we have inherited
    the read() behavior that allows us to read from a List in memory and
    provided a way to map arbitrary streams into objects and added the ability
    to map FieldSets to objects. We will see how to take advantage of this
    next.</para>
  </section>

  <section>
    <title id="infrastructure.1.2">Flat Files</title>

    <para>One of the most common mechanisms for interchanging bulk data has
    always been the flat file. Unlike XML, which has an aggreed upon standard
    for defining how it is structured (XSD), anyone reading a flat file must
    understand ahead of time exactly how the file is structured. In general,
    all flat files fall into two general types: Delimited and Fixed
    Length.</para>

    <section>
      <title>The FieldSet</title>

      <para>When working with flat files in Spring Batch, regardless of
      whether it is for input or output, one of the most important classes is
      the FieldSet. Many architectures and libraries contain abstractions for
      helping you read in from a file, but they usually return a String or an
      array of Strings. This really only gets you halfway there. A FieldSet is
      Spring Batchâ€™s abstraction for enabling the binding of fields from a
      file resource. It allows developers to work with file input in much the
      same way as they would work with database input. A FieldSet is
      conceptually very similar to a Jdbc ResultSet. FieldSets only require
      one argument, a String array of tokens. Optionally you can also
      configure in the names of the fields so that the fields may be accessed
      either by index or name as patterned after ResultSet. In code it means
      it's as simple as:</para>

      <programlisting>String[] tokens = new String[]{"foo", "1", "true"};
FieldSet fs = new DefaultFieldSet(tokens);
String name = fs.readString(0);
int value = fs.readInt(1);
boolean booleanValue = fs.readBoolean(2);</programlisting>

      <para>There are many more options on the FieldSet interface, such as
      Date, long, BigDecimal, etc. The biggest advantage of the FieldSet is
      that it provides consistent parsing of flat file input. Rather than each
      batch job parsing differenty in potentially unexpected ways, it can be
      consistent, both when erroring out due to a format exception, or when
      doing simple data conversions.</para>
    </section>

    <section>
      <title id="infrastructure.1.2.1">FlatFileItemReader</title>

      <para>One of the most common tasks performed in batch jobs involve
      reading from some type of file. A flat file is basically any type of
      file that contains at most two-dimensional (tabular) data. Reading flat
      files in the Spring Batch framework is facilitated by the class
      <emphasis role="bold">FlatFileItemReader</emphasis>, which provides
      basic functionality for reading and parsing flat files. In addition,
      there are default implementations of the <emphasis
      role="bold">Skippable</emphasis> and <emphasis
      role="bold">ItemStream</emphasis> interfaces that solve the majority of
      file processing needs.</para>

      <para>The <emphasis role="bold">FlatFileItemReader</emphasis> class has
      several properties. The three most important of these properties are
      <emphasis role="bold">resource</emphasis>, <emphasis
      role="bold">fieldSetMapper</emphasis> and <emphasis
      role="bold">tokenizer</emphasis>, which define the resource from which
      data will be read and the method by which the read data will be
      converted to distinct fields. The <emphasis
      role="bold">fieldSetMapper</emphasis> and <emphasis
      role="bold">tokenizer</emphasis> interfaces will be explored more in the
      next sections. In addition, we'll explore integration with the file
      system via the resource property. The <emphasis
      role="bold">resource</emphasis> property represents a Spring Core
      <emphasis role="bold">Resource</emphasis>. Documentation explaining how
      to create beans of this type can be found in <ulink
      url="http://static.springframework.org/spring/docs/2.5.x/reference/resources.html"><citetitle>Spring
      Framework, Chapter 4.Resources</citetitle></ulink>. Therefore, this
      guide will not go into the details of creating <emphasis
      role="bold">Resource</emphasis> objects except to make a couple of
      points on the locating files to process within a batch environment.
      Tokenizers and field set mappers will be discussed a bit later.</para>

      <para>As mentioned, the location of the file is defined by the resource
      property. There are only a few methods exposed through a resource
      service. A resource is used to help locate, open, and close resources.
      It can be as simple as: <programlisting>
        Resource resource = new FileSystemResource("resources/trades.csv");
        </programlisting></para>

      <para>In complex batch environments the directory structures are often
      managed by the EAI infrastructure where drop zones for external
      interfaces are established for moving files from ftp locations to batch
      processing locations and vice versa. File moving utilities are beyond
      the scope of the spring batch architecture but it is not unusual for
      batch job streams to include file moving utilities as steps in the job
      stream. It's sufficient to know that the batch architecture only needs
      to know how to locate the files to be processed. Spring Batch begins the
      process of feeding the data into the pipe from this starting
      point.</para>

      <para>The flat file reader uses a ResourceLineReader object to read from
      the file. Optionally, you can specify a <emphasis
      role="bold">RecordSeparatorPolicy</emphasis> through the
      recordSeparatorPolicy property. This can be used to configure more
      low-level features, such as what constitutes the end of a line and
      whether to continue quoted strings over newlines, among other
      things.</para>

      <para>The other properties in the flat file readers allow you to further
      specify how your data will be interpreted: <table>
          <title>Flat File Item Reader Properties</title>

          <tgroup cols="3">
            <colspec align="center" />

            <thead>
              <row>
                <entry align="center">Property</entry>

                <entry align="center">Type</entry>

                <entry align="center">Description</entry>
              </row>
            </thead>

            <tbody>
              <row>
                <entry align="left">encoding</entry>

                <entry align="left">String</entry>

                <entry align="left">Specifies what text encoding to use -
                default is "ISO-8859-1"</entry>
              </row>

              <row>
                <entry align="left">comments</entry>

                <entry align="left">String[]</entry>

                <entry align="left">Specifies line prefixes that indicate
                comment rows</entry>
              </row>

              <row>
                <entry align="left">linesToSkip</entry>

                <entry align="left">int</entry>

                <entry align="left">Number of lines to ignore at the top of
                the file</entry>
              </row>

              <row>
                <entry align="left">firstLineIsHeader</entry>

                <entry align="left">boolean</entry>

                <entry align="left">Indicates that the first line of the file
                is a header containing field names. If the column names have
                not been set yet and the tokenizer extends
                AbstractLineTokenizer, field names will be set automatically
                from this line</entry>
              </row>
            </tbody>
          </tgroup>
        </table></para>

      <section>
        <title>FieldSetMapper</title>

        <para>Field set mappers used by the FlatFileItemReader implement the
        FieldSetMapper interface. This interface defines a single method,
        mapLine, which takes a FieldSet object and maps its contents to some
        Object. This object may be a custom DTO or domain object, or it could
        be as simple as an array, depending on your needs. The <emphasis
        role="bold">FieldSetMapper</emphasis> is used in conjunction with the
        tokenizer to translate a line of data from a resource into an object
        of the desired type:</para>

        <programlisting>public interface FieldSetMapper {
  
  public Object mapLine(FieldSet fs);

}</programlisting>

        <para>As you can see, the pattern used is exatly the same as RowMapper
        used by JdbcTemplate.</para>
      </section>

      <section>
        <title>LineTokenizer</title>

        <para>Because there can be many formats of flat file data, which all
        need to be converted to a FieldSet so that a FieldSetMapper can create
        a useful domain object from them, an abstraction for turning a line of
        input into a FieldSet is necessary. In Spring Batch, this is called
        the LineTokenizer:</para>

        <programlisting>public interface LineTokenizer {
  
  FieldSet tokenize(String line);

}
</programlisting>

        <para>The contract of a LineTokenizer is such that, given a line of
        input (in theory the String could encompass more than one line) a
        FieldSet representing the line will be returned. This will then be
        based to a FieldSetMapper. Spring Batch contains the following
        LineTokenizers:</para>

        <itemizedlist>
          <listitem>
            <para>DelmitedLineTokenizer - Used for files that separate records
            by a delimiter. The most common is a comma, but pipes or
            semicolons are often used as well</para>
          </listitem>

          <listitem>
            <para>FixedLengthTokenizer - Used for tokenizing files where each
            record is separated by a 'fixed width' that must be defined per
            record.</para>
          </listitem>

          <listitem>
            <para>PrefixMatchingCompositeLineTokenizer - Tokenizer that
            determines which among a list of Tokenizers should be used on a
            particular line by checking against a prefix.</para>
          </listitem>
        </itemizedlist>
      </section>

      <section>
        <title>Simple Delimited File Reading Example</title>

        <para>Now that the basic interfaces for reading in flat files have
        been defined, a simple example explaining how they work together is
        helpful. In it's most simple form, the flow when reading a line form a
        file is this:</para>

        <orderedlist>
          <listitem>
            <para>Read one line from the file.</para>
          </listitem>

          <listitem>
            <para>Pass the string line into the LineTokenizer#tokenize()
            method, in order to retrieve a FieldSet</para>
          </listitem>

          <listitem>
            <para>Pass the FieldSet returned from tokenizing to a
            FieldSetMapper, returning the result from the ItemReader#read()
            method</para>
          </listitem>
        </orderedlist>

        <para>The following example will be used to illustrate this using an
        actual domain scenario. This particular batch job reads in football
        players from the following file:<programlisting>ID,lastName,firstName,position,birthYear,debutYear
"AbduKa00,Abdul-Jabbar,Karim,rb,1974,1996",
"AbduRa00,Abdullah,Rabih,rb,1975,1999",
"AberWa00,Abercrombie,Walter,rb,1959,1982",
"AbraDa00,Abramowicz,Danny,wr,1945,1967",
"AdamBo00,Adams,Bob,te,1946,1969",
"AdamCh00,Adams,Charlie,wr,1979,2003"        </programlisting></para>

        <para>We want to map this data to the following Player domain object:
        <programlisting>
          public class Player implements Serializable {
        
          private String ID; 
          private String lastName; 
          private String firstName; 
          private String position; 
          private int birthYear; 
          private int debutYear;
        
          public String toString() {
                
                return "PLAYER:ID=" + ID + ",Last Name=" + lastName + 
                ",First Name=" + firstName + ",Position=" + position + 
                ",Birth Year=" + birthYear + ",DebutYear=" + 
                debutYear;
          }

          // setters and getters...
          }
          </programlisting></para>

        <para>In order to map a FieldSet into our Player object, we need to
        create a FieldSetMapper that returns players:</para>

        <para><programlisting>
        protected static class PlayerFieldSetMapper implements FieldSetMapper {
                public Object mapLine(FieldSet fieldSet) {
                        Player player = new Player();

                        player.setID(fieldSet.readString(0));
                        player.setLastName(fieldSet.readString(1));
                        player.setFirstName(fieldSet.readString(2)); 
                        player.setPosition(fieldSet.readString(3));
                        player.setBirthYear(fieldSet.readInt(4));
                        player.setDebutYear(fieldSet.readInt(5));

                        return player;
                }
        }
      </programlisting></para>

        <para>We can then read in from the filed by correctly constructing our
        FlatFileItemReader and calling read():</para>

        <programlisting>FlatFileItemReader itemReader = new FlatFileItemReader();
itemReader.setResource = new FileSystemResource("resources/players.csv");
//DelimitedLineTokenizer defaults to comma as it's delimiter
itemReader.setLineTokenizer = new DelimitedLineTokenizer();
itemReader.setFieldSetMapper = new PlayerFieldSetMapper();
itemReader.read();
</programlisting>

        <para>Each call to read will return a new Player object from each line
        in the file. When the end of the file is reached, null will be
        returned.</para>
      </section>

      <section>
        <title>Mapping fields by name</title>

        <para>There is one additional functionality that is similar in
        function to a JDBC ResultSet. The names of the fields can be injected
        into the Tokenizer to increase the readability of the mapping
        function. We can expose this behavior by adding the following. First,
        we tell the tokenizer what the names of the fields in the fieldset
        are:</para>

        <para><programlisting>
    tokenizer.setNames(new String[] {"ID", "lastName","firstName","position","birthYear","debutYear"}); 
          </programlisting></para>

        <para>and provide a mapper that uses this information as
        follows:</para>

        <para><programlisting>
    public class PlayerMapper implements FieldSetMapper {
        public Object mapLine(FieldSet fs) {
                        
           if(fs == null){
              return null;
           }
                        
           Player player = new Player();
           player.setID(fs.readString("ID"));
           player.setLastName(fs.readString("lastName"));
           player.setFirstName(fs.readString("firstName"));
           player.setPosition(fs.readString("position"));
           player.setDebutYear(fs.readInt("debutYear"));
           player.setBirthYear(fs.readInt("birthYear"));
                        
           return player;
        }

   }        </programlisting></para>
      </section>

      <section>
        <title>BeanWrapperFieldSetMapper</title>

        <para>For many, having to write a specific FieldSetMapper is equally
        as cumbersome as writing a specific RowMapper for a JdbcTemplate.
        Spring Batch makes this easier by providing a FieldSetMapper that
        automatically maps fields by matching a field name with a setter using
        the JavaBean spec. Again using the footbal example, the FieldSetMapper
        configuration looks like the following:</para>

        <programlisting>&lt;bean id="fieldSetMapper"
      class="org.springframework.batch.io.file.mapping.BeanWrapperFieldSetMapper"&gt;
  &lt;property name="prototypeBeanName" value="player" /&gt;
&lt;/bean&gt;

&lt;bean id="person"
      class="org.springframework.batch.sample.domain.Player"
      scope="prototype" /&gt;</programlisting>

        <para>For each entry in the FieldSet, the mapper will look for a
        corresponding setter on a new instance of the Player object (for this
        reason, prototype scope is required) in the same way the Spring
        container will look for setters matching a property name. Each
        available field in the FieldSet will be mapped, and the resultant
        Player object will be returned, only there was no code
        required.</para>
      </section>

      <section>
        <title>FixedLengthLineTokenizer</title>

        <para>So far only delimited files have been discussed in much detail,
        however, they respresent only half of the file reading picture. Many
        organizations that use flat files use fixed length formats. An example
        field length file is below:</para>

        <programlisting>UK21341EAH4121131.11customer1
UK21341EAH4221232.11customer2
UK21341EAH4321333.11customer3
UK21341EAH4421434.11customer4
UK21341EAH4521535.11customer5</programlisting>

        <para>While this looks like one large field, it actually represent 4
        distinct fields:</para>

        <orderedlist>
          <listitem>
            <para>ISIN: Unique identifier for the item being order - 12
            characters long.</para>
          </listitem>

          <listitem>
            <para>Quantity: Number of this item being ordered - 3 characters
            long.</para>
          </listitem>

          <listitem>
            <para>Price: Price of the item - 4 characters long.</para>
          </listitem>

          <listitem>
            <para>Customer: Id of the customer ordering the item - 8
            characters long.</para>
          </listitem>
        </orderedlist>

        <para>When configuring the FixedLengthLineTokenizer, each of these
        lengths must be provided in the form of ranges:</para>

        <programlisting>&lt;bean id="fixedLengthLineTokenizer"
      class="org.springframework.batch.io.file.transform.FixedLengthTokenizer"&gt;
  &lt;property name="names" value="ISIN, Quantity, Price, Customer" /&gt;
  &lt;property name="columns" value="1-12, 13-15, 16-20, 21-29" /&gt;
&lt;/bean&gt;</programlisting>

        <para>This LineTokenizer will return the same FieldSet as if a
        dlimiter had been used, allowing the same approachs above to be used
        such as the BeanWrapperFieldSetMapper, in a way that is ignorant of
        how the actual line was parsed.</para>
      </section>

      <section>
        <title>PrefixMatchingCompositeLineTokenizer</title>

        <para>All of the file reading examples up to this point have all made
        a key assumption for simplicity's sake: one record equals one line.
        However, this may not always be the case. It's very common that a file
        might have records spanning multiple lines with multiple formats. The
        following excerpt from a file illustrates this:</para>

        <programlisting>HEA;0013100345;2007-02-15
NCU;Smith;Peter;;T;20014539;F
BAD;;Oak Street 31/A;;Small Town;00235;IL;US
SAD;Smith, Elizabeth;Elm Street 17;;Some City;30011;FL;United States
BIN;VISA;VISA-12345678903
LIT;1044391041;37.49;0;0;4.99;2.99;1;45.47
LIT;2134776319;221.99;5;0;7.99;2.99;1;221.87
SIN;UPS;EXP;DELIVER ONLY ON WEEKDAYS
FOT;2;2;267.34</programlisting>

        <para>Everything between the line starting with 'HEA' and the line
        starting with 'FOT' is considered one record. The
        PrefixMatchingCompositeLineTokenizer makes this easier by matching the
        prefix in a line with a particular tokenizer:</para>

        <programlisting>&lt;bean id="orderFileDescriptor"
      class="org.springframework.batch.io.file.transform.PrefixMatchingCompositeLineTokenizer"&gt;
  &lt;property name="tokenizers"&gt;
   &lt;map&gt;
    &lt;entry key="HEA" value-ref="headerRecordDescriptor" /&gt;
    &lt;entry key="FOT" value-ref="footerRecordDescriptor" /&gt;
    &lt;entry key="BCU" value-ref="businessCustomerLineDescriptor" /&gt;
    &lt;entry key="NCU" value-ref="customerLineDescriptor" /&gt;
    &lt;entry key="BAD" value-ref="billingAddressLineDescriptor" /&gt;
    &lt;entry key="SAD" value-ref="shippingAddressLineDescriptor" /&gt;
    &lt;entry key="BIN" value-ref="billingLineDescriptor" /&gt;
    &lt;entry key="SIN" value-ref="shippingLineDescriptor" /&gt;
    &lt;entry key="LIT" value-ref="itemLineDescriptor" /&gt;
    &lt;entry key="" value-ref="defaultLineDescriptor" /&gt;
   &lt;/map&gt;
  &lt;/property&gt;
&lt;/bean&gt;</programlisting>

        <para>This ensures that the line will be parsed correctly, which is
        especially important for fixed length input, with the correct field
        names. Any users of the FlatFileItemReader in this scenario must
        continue calling read() until the footer for the record is returned,
        allowing them to return a complete order as one 'item'.</para>
      </section>
    </section>

    <section>
      <title>FlatFileItemWriter</title>

      <para>Writing out to flat files has the same problems and issues that
      reading in from a file must overcome. It must be able to write out in
      either dlimited or fixed length formats in a transactional
      mannger.</para>

      <section>
        <title>LineAggregator</title>

        <para>Just like file reading's LineTokenizer interface is necessary to
        take a string and split it into tokens, file writing must have a way
        to aggregate multiple fields into a single string for writing to a
        file. In Spring Batch this is the LineAggregator:</para>

        <programlisting>public interface LineAggregator {

 public String aggregate(FieldSet fieldSet);
}
</programlisting>

        <para>The LineAggregator is exactly the opposite of a LineTokenizer.
        LineTokenizer takes a string and returns a FieldSet, wheras
        LineAggreator takes a FieldSet and returns a string. As with reading
        there are two types: DelimitedLineAggregator and
        FixedLengthLineAggregator.</para>
      </section>

      <section>
        <title>FieldSetCreator</title>

        <para>Because the LineAggregator interface uses a FieldSet as it's
        mechanism for converting to a string, there needs to be an interface
        that describes how to convert from an object into a FieldSet:</para>

        <programlisting>public interface FieldSetCreator {

  FieldSet mapItem(Object data);

}</programlisting>

        <para>As with LineTokenizer and LineAggregator, FieldSetCreator is the
        polar opposite of FieldSetMapper. FieldSetMapper takes a FieldSet and
        returns a mapped object, whereas a FieldSetCreator takes an Object and
        returns a FieldSet.</para>
      </section>

      <section>
        <title>Simple Delimited File Writing Example</title>

        <para>Now that both the LineAggregator and FieldSetCreator interfaces
        have been defined, the basic flow of writing can be explained:</para>

        <orderedlist>
          <listitem>
            <para>The object to be written is passed to the FieldSetCreator in
            order to obtain a FieldSet.</para>
          </listitem>

          <listitem>
            <para>The returned FieldSet is passed to the LineAggregator</para>
          </listitem>

          <listitem>
            <para>The returned string is written to the configured
            file.</para>
          </listitem>
        </orderedlist>

        <para>The following excerpt from the FlatFileItemWriter expresses this
        in code:</para>

        <programlisting>public void write(Object data) throws Exception {
  FieldSet fieldSet = fieldSetCreator.mapItem(data);
  getOutputState().write(lineAggregator.aggregate(fieldSet) + LINE_SEPARATOR);
}</programlisting>

        <para>A simple configuration with the smallest ammount of setters
        would look like the following:</para>

        <programlisting>&lt;bean id="itemWriter"
      class="org.springframework.batch.io.file.FlatFileItemWriter"&gt;
  &lt;property name="resource"
            value="file:target/test-outputs/20070122.testStream.multilineStep.txt" /&gt;
  &lt;property name="fieldSetCreator"&gt;
    &lt;bean class="org.springframework.batch.io.file.mapping.PassThroughFieldSetMapper"/&gt;
  &lt;/property&gt;
&lt;/bean&gt;</programlisting>
      </section>

      <section>
        <title>Handling file creation</title>

        <para>FlatFileItemReader has a very simple relationship with file
        resources. When the reader is initialized, it opens the file if it
        exists, and throws an exception if it does not. File writing isn't
        quite so simple. At first glance it seems like a similiar straight
        forward contract should exist for FlatFileItemWriter, if the file
        already exists, throw an exception, if it does not, create it and
        start writing. Job restart throws a bit of a kink into this. In the
        normal restart scenario, the contract is reversed, if the file exists
        start writing to it from the last known good position, if it does not,
        throw an exception. However, what happens if the file name for this
        job is always the same? In this case, you would want to delete the
        file if it exists, unless it's a restart. Because of this possibility,
        the FlatFileItemWriter contains the property, shouldDeleteIfExists.
        Setting this property to true will cause an existing file with the
        same name to be deleted when the writer is opened.</para>
      </section>
    </section>
  </section>

  <section>
    <title id="infrastructure.2.3">XML Item Readers and Writers</title>

    <para>Spring Batch provides transactional infrastructure for both reading
    XML records and mapping them to Java objects as well as writing Java
    objects as XML records.</para>

    <note>
      <title>Constraints on streaming XML</title>

      <para>StAX API is used for I/O as other standard XML APIs do not fit
      batch processing requirements (DOM loads the whole input into memory at
      once and SAX controls the parsing process allowing the user only to
      provide callbacks).</para>
    </note>

    <para>Spring Batch is not tied to any particular OXM technology. Typical
    use is to delegate <ulink
    url="http://static.springframework.org/spring-ws/site/reference/html/oxm.html"><citetitle>OXM
    to Spring WS</citetitle></ulink>, which provides uniform abstraction for
    the most popular OXM technologies. However dependency on Spring WS is
    optional and you can choose to implement Spring Batch specific interfaces
    if desired. The relationship to the technologies that OXM supports can be
    shown as the following:</para>

    <para><mediaobject>
        <imageobject role="fo">
          <imagedata align="center"
                     fileref="../../../../target/site/reference/images/oxm-fragments.png"
                     format="PNG" />
        </imageobject>

        <imageobject role="html">
          <imagedata align="center"
                     fileref="../../resources/reference/images/oxm-fragments.png"
                     format="PNG" />
        </imageobject>

        <caption><para>Figure X: OXM Binding</para></caption>
      </mediaobject></para>

    <para>Lets take a closer look how XML input and output work in batch. It
    is assumed the XML resource is a collection of 'fragments' corresponding
    to individual records. Note that OXM tools are designed to work with
    standalone XML documents rather than XML fragments cut out of an XML
    document, therefore the Spring Batch infrastructure needs to work around
    this fact (as described below).</para>

    <para>On input the reader reads the XML resource until it recognizes a new
    fragment is about to start (by matching the tag name by default). The
    reader creates a standalone XML document from the fragment (or at least
    makes it appear so) and passes the document to a deserializer (typically a
    wrapper around Spring WS Unmarshaller) to map the XML to a Java
    object.</para>

    <para><mediaobject>
        <imageobject role="fo">
          <imagedata align="center"
                     fileref="../../../../target/site/reference/images/xmlinput.png"
                     format="PNG" />
        </imageobject>

        <imageobject role="html">
          <imagedata align="center"
                     fileref="../../resources/reference/images/xmlinput.PNG"
                     format="PNG" />
        </imageobject>

        <caption><para>Figure X: XML Inputs</para></caption>
      </mediaobject></para>

    <para>Output works symetrically to input. Java object is passed to a
    serializer (typically a wrapper around Spring WS Marshaller) which writes
    to output using a custom event writer that filters the StartDocument and
    EndDocument events produced for each fragment by the OXM tools.</para>

    <para>For example configuration of XML input and output see the sample
    xmlStaxJob. //TODO inline the example once it is not subject to change +
    show sample input file</para>
  </section>

  <section>
    <title id="infrastructure.2.2">SQL Sources</title>

    <para>SQL input sources can be configured for various reasons, for
    example:</para>

    <itemizedlist>
      <listitem>
        <para>a staging table for large volumes of sorted data that was loaded
        from flat files</para>
      </listitem>

      <listitem>
        <para>the beginning of an outbound collection of data targeted for an
        external flat file interface</para>
      </listitem>

      <listitem>
        <para>the target of a triggered event like "collect all cases that can
        be automatically closed"</para>

        <para>Spring Batch supports two approaches for accessing a SQL Input
        Source; 1) a cursor driven input source and 2) an indexed based Input
        Query. The cursor driven input source is named because it utilizes a
        jdbc cursor to stream over the SQL input source whereas an indexed
        based input query is designed for easy division of the input into
        ranges</para>
      </listitem>
    </itemizedlist>
  </section>

  <section>
    <title id="infrastructure.5">Validating Input</title>

    <para></para>
  </section>
</chapter>